{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "23248461f9fce"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import ast\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#import necessary modules to work in R within python\n",
    "import rpy2.robjects as r\n",
    "\n",
    "#import pandas2ri for pandas to R dataframe conversion\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "pandas2ri.activate()\n",
    "\n",
    "#this allows me to import libraries in python/R, similar to library('lavaan')\n",
    "from rpy2.robjects.packages import importr\n",
    "utils = importr(\"lavaan\")\n",
    "utils = importr(\"eRm\")\n",
    "\n",
    "#read in the all-outcomes text file\n",
    "outcomes1_txt = open('outcomes.txt')\n",
    "outcomes1_str = outcomes1_txt.read()\n",
    "outcomes1_json = json.loads(outcomes1_str)\n",
    "outcomes1 = pd.DataFrame(outcomes1_json)\n",
    "\n",
    "#read in the assessment data\n",
    "json_file = open('assessment_data.json')\n",
    "json_str = json_file.read()\n",
    "json_data = json.loads(json_str)\n",
    "\n",
    "#import the lti_context_id to root_guid_id report\n",
    "guid_to_context = pd.read_csv('lti_context_to_outcome_guid.csv',encoding='utf-8')\n",
    "guid_to_context = guid_to_context[['RootOutcomeGuid','LtiContextId']]   #I only need these two columns so I get rid of the rest\n",
    "guid_to_context.index = guid_to_context.LtiContextId   #assign LtiContextId as the index (to easily match values)\n",
    "\n",
    "#flatten assessment data down into one level (at the question level)\n",
    "quiz_data = []\n",
    "for submission in json_data:\n",
    "    for question in submission['question_responses']:\n",
    "        quiz_data.append({\n",
    "                            'question_id':question['ident'],\n",
    "                            'score':question['score'],\n",
    "                            'outcome_guid':question['outcome_guid'],\n",
    "                            'quiz_id':submission['data']['quiz_id'],\n",
    "                            'quiz_type':submission['quiz_type'],\n",
    "                            'lti_user_id':submission['lti_user_id'],\n",
    "                            'attempt':submission['data']['attempt'],\n",
    "                            'primary_outcome_guid':submission['primary_outcome_guid'],\n",
    "                            'created_at':submission['created_at'],\n",
    "                            'tc_lti_guid':submission['tc_lti_guid'],\n",
    "                            'lti_context_id':submission['lti_context_id'],\n",
    "                            'enrollment_id':submission['enrollment_id']  \n",
    "                         })\n",
    "\n",
    "#create dataframe from all quiz data\n",
    "df = pd.DataFrame(quiz_data)\n",
    "summative = df[df.quiz_type == 'summative']   #only select summative questions\n",
    "summative = summative[summative.attempt == 0]   #only keep first attempt for each student\n",
    "\n",
    "#create new column that is the outcome and question_id combined, separated by a \":\"\n",
    "summative['outquest_map'] = summative.outcome_guid.astype(str) + ':' + summative.question_id.astype(str)\n",
    "\n",
    "#create new dataframe that has each unique outcome:question_id along with a number from 1 to the length of the outcome:question_id list\n",
    "outquest_to_id_default = pd.DataFrame(pd.Series(range(len(summative.outquest_map.unique()))).astype(str).values,index=summative.outquest_map.unique(),columns=['id'])\n",
    "summative['outquest'] = summative['outquest_map'].map(lambda x: outquest_to_id_default.loc[x,'id'])   #assigns a new id to each question attempt record\n",
    "summative = summative.drop_duplicates()   #drops duplicates in case there are duplicate values\n",
    "\n",
    "#function to get the RootOutcomeGuid from the LtiContextId to Root_Guid file given an LtiContextId\n",
    "#this is trickier than it should have been because there were float values, int values, str values, and series values\n",
    "#this was because some LtiContextIds had multiple RootOutcomeGuids (low enrollment courses)\n",
    "def get_root_guid(cell):\n",
    "    if cell != cell:   #handles None or missing values\n",
    "        return None\n",
    "    else:\n",
    "        if str(cell) in guid_to_context.index:   #checks if the LtiContextId is located in the mapping file\n",
    "            result = guid_to_context.loc[str(cell),'RootOutcomeGuid']   #gets the RootOutcomeGuid\n",
    "            if type(result) == float:   #handles float values\n",
    "                result = str(result)\n",
    "            if len(result) == 2:   #handles list values\n",
    "                try:\n",
    "                    cell_guid = result[0]\n",
    "                except:\n",
    "                    return None\n",
    "            else:                \n",
    "                cell_guid = str(result)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return cell_guid\n",
    "\n",
    "#run the get_root_guid function to get RootOutcomeGuid from Lti_Context_Id\n",
    "summative['root_guid'] = summative['lti_context_id'].map(get_root_guid)\n",
    "\n",
    "#courses in the outcomes file\n",
    "courses_df = outcomes1[outcomes1.root_guid != outcomes1.root_guid]\n",
    "\n",
    "#loop through each course in the assessment data (based on the outcomes without a root_guid, which are the course root_guids)\n",
    "for root_guid_course in range(len(courses_df.guid.values)):\n",
    "    \n",
    "    #extract root_guid and course name\n",
    "    root_guid = courses_df.guid.values[root_guid_course]\n",
    "    course_name = courses_df.short_title.values[root_guid_course]\n",
    "    \n",
    "    #creates an outcome to parent outcome to question mapping dataframe\n",
    "    question_df = summative[summative.root_guid == root_guid][['outcome_guid','primary_outcome_guid','outquest']]\n",
    "    question_df = question_df.drop_duplicates()\n",
    "    \n",
    "    #only keeps outcomes that have the root_guid corresponding to the course\n",
    "    outcomes2 = outcomes1[outcomes1.root_guid == root_guid]\n",
    "    outcomes2 = outcomes2[['guid','parent_guid','short_title','title']]\n",
    "    \n",
    "    #creates a question by student matrix I can use for CFA\n",
    "    #only keeps assessment data with the course root_guid\n",
    "    #questions are columns, rows are students\n",
    "    pivot_table = summative[summative.root_guid == root_guid].pivot_table(index='lti_user_id', columns='outquest', values='score')\n",
    "    \n",
    "    #create a list of students that have at least 5 summative assessments in this specific course\n",
    "    greater_than_five_check = summative[['lti_user_id','quiz_id']].drop_duplicates().groupby('lti_user_id').count()\n",
    "    students = list(greater_than_five_check[greater_than_five_check.quiz_id > 5].index)\n",
    "\n",
    "    #filter out students with low assessment numbers\n",
    "    pivot_table = pivot_table[pivot_table.index.isin(students)]\n",
    "\n",
    "    #initiate lists to save final analysis objects\n",
    "    converge_outcomes = []\n",
    "    no_converge_outcomes = []\n",
    "    less_than_3_questions = []\n",
    "    final_IRT = []\n",
    "    dropped_IRT_questions = []\n",
    "    dropped_CFA_questions = []\n",
    "    normal_rasch_model = []\n",
    "\n",
    "    #number of question responses required\n",
    "    response_threshold = 100\n",
    "\n",
    "    #loop through each enabling outcome for each parent_outcome and run a CFA and IRT model\n",
    "    #we are conducting the analysis at the enabling outcome level\n",
    "    count = 0\n",
    "    for parent_outcome in outcomes2.parent_guid.unique():\n",
    "        outcomes = outcomes2[outcomes2.parent_guid == parent_outcome].guid.unique()\n",
    "        \n",
    "        for outcome in outcomes:    \n",
    "            #uses a unique outcome + questionID identifier because neither outcome nor question is unique by itself\n",
    "            questions2 = question_df[question_df.outcome_guid == outcome].outquest.unique()\n",
    "\n",
    "            #slice pivot_table data based on questions pertaining to outcome\n",
    "            CFA_data = pivot_table[questions2] \n",
    "            \n",
    "            #if there is no data for these questions, skip the outcome\n",
    "            if len(CFA_data) == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            #I don't really know why I drop duplicates here, but it shouldn't hurt anything\n",
    "            CFA_data = CFA_data.T.drop_duplicates().T\n",
    "            \n",
    "            #the thresh parameter means I can make sure each person in the dataset has at least one response to one question\n",
    "            CFA_data = CFA_data.dropna(axis=0,thresh=1)\n",
    "            \n",
    "            #these are the questions left after I drop all students with missing values on all problems\n",
    "            #questions should essentially be the same as questions2\n",
    "            questions = [x for x in questions2 if x in CFA_data.columns]\n",
    "\n",
    "            #remove questions with less than 100 responses\n",
    "            question_count = CFA_data.count()\n",
    "            greater_than_100_questions = question_count[question_count.values > response_threshold].index\n",
    "            CFA_data = CFA_data[greater_than_100_questions]\n",
    "\n",
    "            #questions that were removed for too few responses\n",
    "            removed_questions = list(question_count[question_count.values <= 100].index)\n",
    "            dropped_CFA_questions.append(removed_questions)\n",
    "            dropped_CFA_questions.append([x for x in questions2 if x not in CFA_data.columns])\n",
    "            \n",
    "            #IRT model to identify difficulty for each item\n",
    "            IRT_data = CFA_data.copy()\n",
    "\n",
    "            #throw out questions without 0's or 1's because it messes up the algorithm\n",
    "            #I have to iterate through the dataset 5 times because if I drop students (didn't respond to two items only one)\n",
    "            #then questions might not have a 1 or 0 anymore. And if I drop questions, students might not have more than two\n",
    "            #responses on questions. So it's basically a brute force optimization algorithm to get correct data\n",
    "            for x in range(5):\n",
    "                IRT_data = IRT_data[IRT_data.columns[(IRT_data.min() == 0).values&(IRT_data.max() == 1).values]]\n",
    "                IRT_data = IRT_data.dropna(axis=0,thresh=2)\n",
    "                if len(IRT_data) == 0: #we don't have any IRT data. Break out of the for loop.\n",
    "                    continue\n",
    "                IRT_data = IRT_data.T.drop_duplicates().T\n",
    "\n",
    "            if len(IRT_data) == 0:   #we don't have any IRT data, so skip running everything\n",
    "                continue\n",
    "            \n",
    "            #convert IRT_data into an R object\n",
    "            r_IRT_data = pandas2ri.py2ri(IRT_data)\n",
    "\n",
    "            #dropped IRT questions\n",
    "            dropped_IRT_question = [x for x in CFA_data.columns if x not in IRT_data.columns]\n",
    "            if len(IRT_data.columns) < 3:\n",
    "                for x in IRT_data:\n",
    "                    dropped_IRT_question.append(x)\n",
    "                \n",
    "                continue #stop if there aren't enough questions to run a model\n",
    "\n",
    "            #running the model\n",
    "            try:\n",
    "                r.r('''res = LPCM({}, groupvec = 1, se = TRUE, sum0 = TRUE)'''.format(r_IRT_data.r_repr()))\n",
    "            except:\n",
    "                #if it doesn't work, we probably need a normal rasch model instead of a partial credit model\n",
    "                normal_rasch_model.append({\n",
    "                        'outcome':outcome,\n",
    "                        'parent_outcome':parent_outcome\n",
    "                        })\n",
    "\n",
    "#                 #for now I am stopping the script. Eventually I will have it run a rasch model, if needed.\n",
    "#                 for xyz in IRT_data.columns:\n",
    "#                     dropped_IRT_questions.append(xyz)\n",
    "#                 continue\n",
    "#                 for x in IRT_data.columns:\n",
    "#                     IRT_data = IRT_data[~((IRT_data[x] > 0)&(IRT_data[x] < 1))]\n",
    "#                 IRT_data = IRT_data.dropna(axis=0,thresh=2)\n",
    "#                 r_IRT_data = pandas2ri.py2ri(IRT_data)\n",
    "#                 r.r('''res = RM(as.matrix({}), se = TRUE, sum0 = TRUE)'''.format(r_IRT_data.r_repr()))\n",
    "#                 print \"Abnormal\"\n",
    "\n",
    "            #get means, min, and max for dropped questions for diagnostic purposes\n",
    "            dropped_means = [CFA_data[x].mean() for x in dropped_IRT_question]\n",
    "            dropped_min = [CFA_data[x].min() for x in dropped_IRT_question]\n",
    "            dropped_max = [CFA_data[x].max() for x in dropped_IRT_question]\n",
    "\n",
    "            #save the mean score of the dropped questions\n",
    "            dropped_IRT_questions.append({\n",
    "                    'dropped_IRT_question':dropped_IRT_question,\n",
    "                    'dropped_means':dropped_means,\n",
    "                    'dropped_min':dropped_min,\n",
    "                    'dropped_max':dropped_max,\n",
    "                    'outcome':outcome\n",
    "                })\n",
    "\n",
    "            #extracting coefficients from the model\n",
    "            #question_difficulty is actually question_easiness\n",
    "            question_difficulty = list(r.r('''res['betapar']''')[0])\n",
    "            etapar = list(r.r('''res['etapar']''')[0])\n",
    "            questionsIRT = list(IRT_data.columns)   #questions used in the IRT model\n",
    "\n",
    "            #final results\n",
    "            IRT_results = {'qIRT':questionsIRT,\n",
    "                           'questions':questions,\n",
    "                           'question_difficulty':question_difficulty,\n",
    "                           'dropped_IRT_questions':dropped_IRT_questions,\n",
    "                           'etapar':etapar,\n",
    "                           'outcome':outcome}\n",
    "            final_IRT.append(IRT_results)\n",
    "\n",
    "            #if we have less than 4 items per outcome we have to run a special CFA model (with constraints in order for it to run)\n",
    "            #we are simply saving these outcomes right now\n",
    "            if len(CFA_data.columns) < 4:\n",
    "                less_than_3_questions.append({\n",
    "                        'outcome':outcome,\n",
    "                        'parent_outcome':parent_outcome,\n",
    "                        'number_of_questions':len(CFA_data.columns)\n",
    "                    })\n",
    "                count += 1\n",
    "                continue #stop because we can't run a CFA\n",
    "\n",
    "            #R adds X's before these variables when the dataframe is transferred from pandas to R.\n",
    "            IVs = 'X' + ' + X'.join(CFA_data.columns)\n",
    "            DV = '{} =~ '.format('outcome')\n",
    "            \n",
    "            #create a string formula to use in the CFA\n",
    "            formula = str(DV + IVs)\n",
    "\n",
    "            #creates an R dataframe out of a pandas dataframe\n",
    "            r_CFA_data = pandas2ri.py2ri(CFA_data)\n",
    "\n",
    "            #runs a CFA using the lavaan package\n",
    "            #right now I am catching all errors and just stopping if it doesn't work\n",
    "            try:\n",
    "                r.r('''test <- cfa('{}', data={}, missing = \"ML\",std.lv=TRUE)'''.format(formula, r_CFA_data.r_repr()))\n",
    "                r.r('''summary(test,standardized=TRUE)''')\n",
    "\n",
    "                #extract all fit statistics and parameter estimates from the model\n",
    "                aic = r.r('''AIC(test)''')[0]\n",
    "                bic = r.r('''BIC(test)''')[0]\n",
    "                rmsea = r.r('''fitMeasures(test, \"rmsea\")''')[0]\n",
    "                cfi = r.r('''fitMeasures(test, \"cfi\")''')[0]\n",
    "                tli = r.r('''fitMeasures(test, \"tli\")''')[0]\n",
    "                srmr = r.r('''fitMeasures(test, \"srmr\")''')[0]\n",
    "                parameter_estimates = r.r('''parameterEstimates(test)''')\n",
    "                mod_indices = r.r('''modindices(test)''')\n",
    "                converge_outcomes.append({\n",
    "                        'aic':aic,\n",
    "                        'bic':bic,\n",
    "                        'rmsea':rmsea,\n",
    "                        'cfi':cfi,\n",
    "                        'tli':tli,\n",
    "                        'srmr':srmr,\n",
    "                        'parameter_estimates':parameter_estimates,\n",
    "                        'mod_indices':mod_indices,\n",
    "                        'CFA_data':CFA_data.count(),\n",
    "                        'outcome':outcome,\n",
    "                        'parent_outcome':parent_outcome\n",
    "                    })\n",
    "                count += 1\n",
    "            except:\n",
    "                #there was some error in running the model\n",
    "                count += 1\n",
    "                warnings = r.r('''warnings()''')\n",
    "                no_converge_outcomes.append({\n",
    "                        'warnings':warnings,\n",
    "                        'CFA_data':CFA_data.count(),\n",
    "                        'outcome':outcome,\n",
    "                        'parent_outcome':parent_outcome\n",
    "                    })\n",
    "\n",
    "    #creates a dataframe from CFA converged models\n",
    "    converge_df = pd.DataFrame(converge_outcomes)\n",
    "\n",
    "    if len(converge_df) == 0:   #if we don't have any data, skip returning results\n",
    "        continue\n",
    "    \n",
    "    #this function, when vectorized, will take pandas series and assess model fit quality\n",
    "    #based on four cutoff points (cfi,tli,rmsea,srmr)\n",
    "    def assess_quality(cfi,tli,rmsea,srmr):\n",
    "        count = 0\n",
    "        if cfi > .9:\n",
    "            count += 1\n",
    "        if tli > .9:\n",
    "            count += 1\n",
    "        if rmsea < .08:\n",
    "            count += 1\n",
    "        if srmr < .08:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    #this will vectorize the function to be used on a pandas series\n",
    "    assess_quality = np.vectorize(assess_quality)\n",
    "    \n",
    "    #run the assess_quality function to get a number for model fit\n",
    "    converge_df['model_fit'] = assess_quality(converge_df.cfi,converge_df.tli,converge_df.rmsea,converge_df.srmr)\n",
    "\n",
    "    #model fit summary for this course\n",
    "    #Excellent model fit: 4 means all 4 statistics meet the rules of thumb (CFI>.9,TLI>.9,SRMR<.08,RMSEA<.08)\n",
    "    #Good model fit: 3 means the majority still show good model fit\n",
    "    #Questionable model fit: 2 means it could go either way, improvements needed\n",
    "    #Poor model fit: 1 or 0 means there is no evidence to suggest the model is fitting well\n",
    "\n",
    "    #lambda functions to get the number of questions and their IDs for each analysis\n",
    "    converge_df['questionIDs'] = converge_df['CFA_data'].map(lambda x: list(x.index))\n",
    "    converge_df['question_count'] = converge_df['CFA_data'].map(lambda x: len(list(x.index)))\n",
    "\n",
    "    #rename guid to outcome so I can merge it with my final dataset\n",
    "    outcomes2.rename(columns={'guid':'outcome'},inplace=True)\n",
    "\n",
    "    #this is the final outcomes level data file\n",
    "    final_outcome_level_data = converge_df.copy()\n",
    "\n",
    "    #final outcome level data merged together with other data\n",
    "    final_outcome_level_data = pd.merge(final_outcome_level_data,outcomes2)\n",
    "\n",
    "    #we don't need all columns, only keep the columns you want\n",
    "    final_outcome_level_data = final_outcome_level_data[['parent_outcome','outcome','short_title','cfi','tli','srmr','rmsea','aic','bic','model_fit','question_count','questionIDs']]\n",
    "    \n",
    "    ###outcome level data is completed, next is question level data###\n",
    "    \n",
    "    #getting question level data\n",
    "    questions = []\n",
    "    response_count = []\n",
    "    parameter_estimates = []\n",
    "    \n",
    "    #weird list comprehensions to get questions and response_counts into lists\n",
    "    #these should probably just be normal functions, although the current implementation works\n",
    "    converge_df['CFA_data'].map(lambda x: [questions.append(y) for y in x.index])\n",
    "    converge_df['CFA_data'].map(lambda x: [response_count.append(y) for y in x.values])\n",
    "    converge_df['parameter_estimates'].map(lambda x: parameter_estimates.append(x[x['op'] == '=~'][['rhs','est','se','z','pvalue']]))\n",
    "\n",
    "    #initialize a blank dataframe for use in the following section\n",
    "    CFA_question_data = pd.DataFrame(0,columns=['rhs','est','se','z','pvalue'],index=range(0))\n",
    "    for x in parameter_estimates:   #add parameter estimates to CFA_question_data\n",
    "        CFA_question_data.append(x)\n",
    "\n",
    "    CFA_question_data['rhs'] = CFA_question_data['rhs'].map(lambda x: x[1:])   #parsing the rhs value\n",
    "    \n",
    "    #renaming values to be more easily interpreted\n",
    "    CFA_question_data.rename(columns={'rhs':'question',\n",
    "                          'est':'factor_loading',\n",
    "                          'se':'se_factor_loading',\n",
    "                          'z':'z_factor_loading',\n",
    "                          'pvalue':'p_factor_loading'},inplace=True)\n",
    "    \n",
    "    #reset the index so you can use it in the next analyses\n",
    "    CFA_question_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    questions_IRT = []\n",
    "    difficulty_IRT = []\n",
    "    theta_scales = []\n",
    "    \n",
    "    #weird list comprehensions to get IRT data into a dataframe\n",
    "    #there is definitely a better way\n",
    "    irtdf = pd.DataFrame(final_IRT)\n",
    "    irtdf.qIRT.map(lambda x: [questions_IRT.append(y) for y in x])\n",
    "    irtdf.question_difficulty.map(lambda x: [difficulty_IRT.append(y) for y in x])\n",
    "    irtdf.etapar.map(lambda x: [theta_scales.append(y) for y in x])\n",
    "\n",
    "    #create IRT dataframe from questions and difficulty parameter\n",
    "    IRT_question_data = pd.DataFrame(list(zip(questions_IRT,difficulty_IRT))).rename(columns={0:'question',1:'difficulty'})\n",
    "\n",
    "    #create CFA dataframe\n",
    "    CFA_question_data2 = pd.DataFrame(list(zip(questions,response_count))).rename(columns={0:'question',1:'response_count'})\n",
    "\n",
    "    #merge IRT, CFA, and missing CFA data together into a question_level_data dataframe\n",
    "    question_level_data = IRT_question_data.merge(CFA_question_data2,on='question',how='outer')\n",
    "    question_level_data = question_level_data.merge(CFA_question_data,on='question',how='outer')\n",
    "    \n",
    "    ###Final question level data###\n",
    "    final_question_level_data = question_level_data.sort_values('difficulty')\n",
    "\n",
    "    ####Final Output Files####\n",
    "    # final_question_level_data\n",
    "    # final_outcome_level_data\n",
    "    # no_converge_outcomes\n",
    "    # dropped_IRT_questions\n",
    "    # dropped_CFA_questions\n",
    "\n",
    "    #code found here: http://stackoverflow.com/questions/26068021/iterate-over-rows-and-expand-pandas-dataframe\n",
    "    #flatten a dataframe when you have a series of lists for each item in the list in the series\n",
    "    dfIn = final_outcome_level_data.copy()\n",
    "    dfIn.loc[:, 'questionIDs'] = dfIn.questionIDs.apply(np.atleast_1d)\n",
    "    all_locations = np.hstack(dfIn.questionIDs)\n",
    "    all_names = np.hstack([[n]*len(l) for n, l in dfIn[['outcome','questionIDs']].values])\n",
    "    dfOut = pd.DataFrame({'question':all_locations, 'outcome':all_names})\n",
    "\n",
    "    #output is the final merge of all question/outcome datafiles\n",
    "    output = dfOut.merge(final_outcome_level_data,on='outcome',how='outer').merge(final_question_level_data,on='question',how='outer')\n",
    "    \n",
    "    #add min, max, and mean to dropped IRT questions for diagnostic purposes\n",
    "    for outcome in dropped_IRT_questions:\n",
    "        for question in range(len(outcome['dropped_IRT_question'])):\n",
    "            questionID = outcome['dropped_IRT_question'][question]\n",
    "            questionmax = outcome['dropped_max'][question]\n",
    "            questionmin = outcome['dropped_min'][question]\n",
    "            questionmean = outcome['dropped_means'][question]\n",
    "            output.loc[output.question == questionID,'question_min'] = questionmin\n",
    "            output.loc[output.question == questionID,'question_max'] = questionmax\n",
    "\n",
    "    #add in the actual questionID since we replaced it with the question/outcome combo ID\n",
    "    outquest_to_id = outquest_to_id_default.copy()\n",
    "    outquest_to_id['outquest'] = outquest_to_id.index\n",
    "    outquest_to_id = outquest_to_id.set_index('id')\n",
    "    outquest_to_id['questionID'] = outquest_to_id.outquest.map(lambda x: x.split(':')[1])\n",
    "    output['questionID'] = output.question.map(lambda x: outquest_to_id.loc[x,'questionID'])\n",
    "\n",
    "    #merge with mean_correct dataset to get mean correct values\n",
    "    mean_correct_df = summative.groupby('question_id').mean()\n",
    "    mean_correct_df['questionID'] = mean_correct_df.index\n",
    "    mean_correct_df = mean_correct_df[['questionID','score']]\n",
    "    output = output.merge(mean_correct_df,on='questionID')\n",
    "    \n",
    "    #merge with attempts dataset to get attempts for each question\n",
    "    attempts_df = summative.groupby('question_id').count()\n",
    "    attempts_df['questionID'] = attempts_df.index\n",
    "    attempts_df = attempts_df[['questionID','score']]\n",
    "    output = output.merge(attempts_df,on='questionID')\n",
    "    \n",
    "    \n",
    "    #output the data to the root directory\n",
    "    output.to_csv('Output//{}_{}_final_IRT-CFA_data.csv'.format(course_name,root_guid[:4]),index=False,encoding='utf-8')\n",
    "\n",
    "    #adds outcomes to all questions (even if they didn't converge for CFA)\n",
    "    outquest_to_id['outcome'] = outquest_to_id.outquest.map(lambda x: x.split(':')[0])\n",
    "    output['outcome'] = output.question.map(lambda x: outquest_to_id.loc[x,'outcome'])\n",
    "\n",
    "    #prepare dropped CFA questions for export\n",
    "    CFA_questions = []\n",
    "    for question_list in dropped_CFA_questions:\n",
    "        for question in question_list:\n",
    "            CFA_questions.append(question)\n",
    "\n",
    "    final_CFA_list = []\n",
    "    for question in CFA_questions:\n",
    "        if question not in output.question.values:\n",
    "            final_CFA_list.append(question)\n",
    "\n",
    "    #prepare dropped IRT questions for export\n",
    "    IRT_questions = []\n",
    "    for question_list in dropped_IRT_questions:\n",
    "        for question in question_list['dropped_IRT_question']:\n",
    "            IRT_questions.append(question)\n",
    "\n",
    "    final_IRT_list = []\n",
    "    for question in IRT_questions:\n",
    "        if question not in output.question.values:\n",
    "            final_IRT_list.append(question)\n",
    "\n",
    "    #export data to csv files named by course name and the first 4 characters of the root guid\n",
    "    pivot_table[final_CFA_list].describe().T.to_csv('Output//{}_{}_dropped_CFA_questions.csv'.format(course_name,root_guid[:4]))\n",
    "    if len(final_IRT_list) < 1:\n",
    "        pass\n",
    "    else:\n",
    "        pivot_table[final_IRT_list].describe().T.to_csv('Output//{}_{}_dropped_IRT_questions.csv'.format(course_name,root_guid[:4]))\n",
    "    pd.DataFrame(no_converge_outcomes).to_csv('Output//{}_{}_no_converge_outcomes.csv'.format(course_name,root_guid[:4]))\n",
    "    pd.DataFrame(less_than_3_questions).to_csv('Output//{}_{}_less_than_3_questions.csv'.format(course_name,root_guid[:4]))\n",
    "    pd.DataFrame(normal_rasch_model).to_csv('Output//{}_{}_normal_rasch_model.csv'.format(course_name,root_guid[:4]))"
   ]
  }
 ],
 "metadata": {
  "comet_paths": [],
  "comet_tracking": true,
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
